# Testes do Sistema de ClassificaÃ§Ã£o de Discurso de Ã“dio

## ğŸ“‹ VisÃ£o Geral

Este diretÃ³rio contÃ©m todos os testes automatizados do projeto, implementados com **PyTest**. Os testes garantem que o modelo atenda aos requisitos mÃ­nimos de desempenho e que a API funcione corretamente.

## ğŸ¯ Tipos de Testes

### 1. **Testes de Desempenho do Modelo** (`test_model_performance.py`)
Valida se o modelo atende aos requisitos mÃ­nimos de qualidade estabelecidos.

**MÃ©tricas e Thresholds:**
- **AcurÃ¡cia mÃ­nima**: 65%
- **PrecisÃ£o mÃ­nima**: 60%
- **Recall mÃ­nimo**: 60%
- **F1-Score mÃ­nimo**: 60%

**ValidaÃ§Ãµes incluÃ­das:**
- âœ… Carregamento correto do modelo
- âœ… Tamanho adequado do dataset de teste
- âœ… MÃ©tricas dentro dos thresholds estabelecidos
- âœ… Desempenho balanceado entre classes
- âœ… AnÃ¡lise de falsos positivos e falsos negativos

### 2. **Testes de IntegraÃ§Ã£o** (`test_controllers.py`)
Testa os endpoints da API Flask.

**Endpoints testados:**
- `GET /` - PÃ¡gina inicial
- `GET /health` - Health check
- `GET /model-info` - InformaÃ§Ãµes do modelo
- `POST /predict` - PrediÃ§Ã£o individual
- `POST /predict/batch` - PrediÃ§Ã£o em lote

### 3. **Testes UnitÃ¡rios** (`test_model_service.py`)
Testa as funcionalidades do serviÃ§o de modelo.

**Funcionalidades testadas:**
- Carregamento do modelo
- PrediÃ§Ãµes individuais e em lote
- CÃ¡lculo de confianÃ§a
- Tratamento de erros

## ğŸš€ Como Executar os Testes

### PrÃ©-requisitos
```bash
pip install -r requirements.txt
```

### Executar todos os testes
```bash
python run_tests.py
```

### Executar apenas testes de desempenho
```bash
python run_tests.py performance
```

### Executar com cobertura de cÃ³digo
```bash
python run_tests.py coverage
```

### Executar teste especÃ­fico
```bash
python run_tests.py -k test_model_accuracy_threshold
```

### Comandos PyTest diretos
```bash
# Todos os testes
pytest

# Apenas testes de desempenho
pytest tests/test_model_performance.py -v

# Com marcadores
pytest -m performance    # Apenas testes de performance
pytest -m unit          # Apenas testes unitÃ¡rios
pytest -m integration   # Apenas testes de integraÃ§Ã£o

# Com cobertura
pytest --cov=. --cov-report=html
```

## ğŸ” InterpretaÃ§Ã£o dos Resultados

### Testes de Desempenho

Se os testes de desempenho **FALHAREM**, significa que:
- âŒ O modelo **NÃƒO** atende aos requisitos mÃ­nimos de qualidade
- âŒ O modelo **NÃƒO DEVE** ser implantado em produÃ§Ã£o
- âŒ Ã‰ necessÃ¡rio retreinar ou ajustar o modelo

### Exemplo de saÃ­da:
```
=== RELATÃ“RIO DE DESEMPENHO DO MODELO ===

MÃ©tricas Gerais:
- AcurÃ¡cia: 67.89% (threshold: 65.00%)
- PrecisÃ£o: 65.38% (threshold: 60.00%)
- Recall: 64.49% (threshold: 60.00%)
- F1-Score: 64.93% (threshold: 60.00%)

Dataset de Teste:
- Total de amostras: 600
- DistribuiÃ§Ã£o de classes: [400, 200]

Status: âœ… APROVADO
```

## ğŸ“Š Arquivos de ConfiguraÃ§Ã£o

### `pytest.ini`
ConfiguraÃ§Ãµes globais do PyTest, incluindo:
- DiretÃ³rios de teste
- Marcadores customizados
- OpÃ§Ãµes padrÃ£o
- Timeout

### `conftest.py`
ConfiguraÃ§Ãµes compartilhadas entre testes:
- Fixtures globais
- Hooks personalizados
- RelatÃ³rios customizados

## ğŸ›¡ï¸ Garantia de Qualidade

Os testes implementados garantem:

1. **Qualidade do Modelo**: ValidaÃ§Ã£o contÃ­nua das mÃ©tricas de desempenho
2. **Estabilidade da API**: Todos os endpoints funcionam corretamente
3. **Tratamento de Erros**: Sistema responde adequadamente a entradas invÃ¡lidas
4. **Barreira de SeguranÃ§a**: Impede deploy de modelos inadequados

## ğŸ”„ CI/CD

Estes testes devem ser executados:
- Antes de cada commit (pre-commit hook)
- Em cada pull request
- Antes de cada deploy
- ApÃ³s retreinar o modelo

## ğŸ“ Notas Importantes

1. **Dados de Teste**: Os testes usam 20% do dataset `hate.csv` como dados de teste
2. **Reprodutibilidade**: Random seed fixo (42) garante resultados consistentes
3. **Timeout**: Testes tÃªm timeout de 5 minutos para evitar travamentos
4. **ParalelizaÃ§Ã£o**: Testes podem ser executados em paralelo com `pytest -n auto` 